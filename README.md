# A playground implementing a simple bigram language model

### Model implements

1. Self attention
2. Multi headed attention
3. Decoder of the transformer block
4. Character level tokenization

Everything is implemented from scratch

- Based on the paper [attention is all you need]("https://arxiv.org/abs/1706.03762").
