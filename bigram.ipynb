{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n",
      "Running on MPS Device\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "  mps_device = torch.device(\"mps\")\n",
    "  x = torch.ones(1, device=mps_device)\n",
    "  print(x)\n",
    "  print(\"Running on MPS Device\")\n",
    "else:\n",
    "  print(\"MPS device not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 100000\n",
    "eval_iters = 200\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text file =  1115393\n"
     ]
    }
   ],
   "source": [
    "print(\"length of text file = \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character level language model\n",
    "# Encode characters to numbers\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # take a string output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n"
     ]
    }
   ],
   "source": [
    "print(encode (\"hii there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(decode (encode (\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003853 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9* len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), output is 47\n",
      "When input is tensor([18, 47]), output is 56\n",
      "When input is tensor([18, 47, 56]), output is 57\n",
      "When input is tensor([18, 47, 56, 57]), output is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), output is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), output is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), output is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), output is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print(f\"When input is {context}, output is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) tensor([47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 7, 5, 6, 4])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(10,(5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]], device='mps:0')\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]], device='mps:0')\n",
      "---\n",
      "when input is [53], the target is: 59\n",
      "when input is [53, 59], the target is: 6\n",
      "when input is [53, 59, 6], the target is: 1\n",
      "when input is [53, 59, 6, 1], the target is: 58\n",
      "when input is [53, 59, 6, 1, 58], the target is: 56\n",
      "when input is [53, 59, 6, 1, 58, 56], the target is: 47\n",
      "when input is [53, 59, 6, 1, 58, 56, 47], the target is: 40\n",
      "when input is [53, 59, 6, 1, 58, 56, 47, 40], the target is: 59\n",
      "when input is [49], the target is: 43\n",
      "when input is [49, 43], the target is: 43\n",
      "when input is [49, 43, 43], the target is: 54\n",
      "when input is [49, 43, 43, 54], the target is: 1\n",
      "when input is [49, 43, 43, 54, 1], the target is: 47\n",
      "when input is [49, 43, 43, 54, 1, 47], the target is: 58\n",
      "when input is [49, 43, 43, 54, 1, 47, 58], the target is: 1\n",
      "when input is [49, 43, 43, 54, 1, 47, 58, 1], the target is: 58\n",
      "when input is [13], the target is: 52\n",
      "when input is [13, 52], the target is: 45\n",
      "when input is [13, 52, 45], the target is: 43\n",
      "when input is [13, 52, 45, 43], the target is: 50\n",
      "when input is [13, 52, 45, 43, 50], the target is: 53\n",
      "when input is [13, 52, 45, 43, 50, 53], the target is: 8\n",
      "when input is [13, 52, 45, 43, 50, 53, 8], the target is: 0\n",
      "when input is [13, 52, 45, 43, 50, 53, 8, 0], the target is: 26\n",
      "when input is [1], the target is: 39\n",
      "when input is [1, 39], the target is: 1\n",
      "when input is [1, 39, 1], the target is: 46\n",
      "when input is [1, 39, 1, 46], the target is: 53\n",
      "when input is [1, 39, 1, 46, 53], the target is: 59\n",
      "when input is [1, 39, 1, 46, 53, 59], the target is: 57\n",
      "when input is [1, 39, 1, 46, 53, 59, 57], the target is: 43\n",
      "when input is [1, 39, 1, 46, 53, 59, 57, 43], the target is: 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 \n",
    "block_size = 8\n",
    "\n",
    "def get_batches(split):\n",
    "  # generate a small batch of data of inputs and targets y\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([ data[i  : i+block_size] for i in ix])\n",
    "  y = torch.stack([ data[i+1: i+block_size+1] for i in ix])\n",
    "  x,y = x.to(mps_device), y.to(mps_device)\n",
    "  return x,y\n",
    "\n",
    "xb,yb = get_batches('train')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range (batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = xb[b, : t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f\"when input is {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "  out = {}\n",
    "  m.eval()\n",
    "  for split in ['train', 'val']:\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "      X,Y = get_batches(split)\n",
    "      logits, loss = model (X,Y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  m.train()\n",
    "  return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8948, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "UNasE3QKdYMjKfxcq-PyQbRF.\n",
      "jxuUfZWievNL:C&v-jkcECOIiyeg zbZAcQ?yObr&MkzeAmyFXSPHd,j&?oneOAvrFotKuLTDx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    logits = self.token_embedding_table(idx) # (batch, time, channel)\n",
    "    \n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B,T,C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B,T)  array of indices in the current context\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self(idx)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(mps_device)\n",
    "logits,loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "context = torch.zeros((1,1) , dtype=torch.long, device=mps_device) \n",
    "print(decode(m.generate( context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7331 val loss 4.7272\n",
      "step 300: train loss 4.5009 val loss 4.5134\n",
      "step 600: train loss 4.3064 val loss 4.2910\n",
      "step 900: train loss 4.1099 val loss 4.1279\n",
      "step 1200: train loss 3.9290 val loss 3.9131\n",
      "step 1500: train loss 3.7882 val loss 3.7971\n",
      "step 1800: train loss 3.6375 val loss 3.6273\n",
      "step 2100: train loss 3.4978 val loss 3.5211\n",
      "step 2400: train loss 3.3808 val loss 3.4120\n",
      "step 2700: train loss 3.2777 val loss 3.3131\n",
      "step 3000: train loss 3.2137 val loss 3.2193\n",
      "step 3300: train loss 3.1012 val loss 3.1650\n",
      "step 3600: train loss 3.0703 val loss 3.0901\n",
      "step 3900: train loss 2.9990 val loss 3.0108\n",
      "step 4200: train loss 2.9242 val loss 2.9351\n",
      "step 4500: train loss 2.9139 val loss 2.9165\n",
      "step 4800: train loss 2.8583 val loss 2.8723\n",
      "step 5100: train loss 2.8057 val loss 2.8369\n",
      "step 5400: train loss 2.7822 val loss 2.7876\n",
      "step 5700: train loss 2.7357 val loss 2.7726\n",
      "step 6000: train loss 2.7508 val loss 2.7329\n",
      "step 6300: train loss 2.6784 val loss 2.7318\n",
      "step 6600: train loss 2.6746 val loss 2.6916\n",
      "step 6900: train loss 2.6721 val loss 2.6687\n",
      "step 7200: train loss 2.6230 val loss 2.6533\n",
      "step 7500: train loss 2.6366 val loss 2.6335\n",
      "step 7800: train loss 2.6206 val loss 2.6251\n",
      "step 8100: train loss 2.5733 val loss 2.5650\n",
      "step 8400: train loss 2.5955 val loss 2.6019\n",
      "step 8700: train loss 2.5926 val loss 2.5886\n",
      "step 9000: train loss 2.5542 val loss 2.5585\n",
      "step 9300: train loss 2.5595 val loss 2.5804\n",
      "step 9600: train loss 2.5459 val loss 2.5552\n",
      "step 9900: train loss 2.5201 val loss 2.5474\n",
      "step 10200: train loss 2.5224 val loss 2.5665\n",
      "step 10500: train loss 2.5438 val loss 2.5295\n",
      "step 10800: train loss 2.5298 val loss 2.5391\n",
      "step 11100: train loss 2.5279 val loss 2.5343\n",
      "step 11400: train loss 2.5195 val loss 2.5179\n",
      "step 11700: train loss 2.5220 val loss 2.5310\n",
      "step 12000: train loss 2.5094 val loss 2.5180\n",
      "step 12300: train loss 2.5109 val loss 2.5002\n",
      "step 12600: train loss 2.4955 val loss 2.5230\n",
      "step 12900: train loss 2.5059 val loss 2.5163\n",
      "step 13200: train loss 2.5050 val loss 2.4727\n",
      "step 13500: train loss 2.4775 val loss 2.4919\n",
      "step 13800: train loss 2.4778 val loss 2.5198\n",
      "step 14100: train loss 2.4988 val loss 2.5086\n",
      "step 14400: train loss 2.4942 val loss 2.4917\n",
      "step 14700: train loss 2.4919 val loss 2.5191\n",
      "step 15000: train loss 2.5202 val loss 2.5068\n",
      "step 15300: train loss 2.4889 val loss 2.4915\n",
      "step 15600: train loss 2.5088 val loss 2.5014\n",
      "step 15900: train loss 2.4916 val loss 2.4820\n",
      "step 16200: train loss 2.4696 val loss 2.4943\n",
      "step 16500: train loss 2.4716 val loss 2.4918\n",
      "step 16800: train loss 2.4559 val loss 2.5006\n",
      "step 17100: train loss 2.4860 val loss 2.4925\n",
      "step 17400: train loss 2.4877 val loss 2.5010\n",
      "step 17700: train loss 2.4908 val loss 2.4842\n",
      "step 18000: train loss 2.4968 val loss 2.4805\n",
      "step 18300: train loss 2.4688 val loss 2.4832\n",
      "step 18600: train loss 2.4690 val loss 2.4714\n",
      "step 18900: train loss 2.4764 val loss 2.4936\n",
      "step 19200: train loss 2.4696 val loss 2.4968\n",
      "step 19500: train loss 2.4706 val loss 2.4798\n",
      "step 19800: train loss 2.4687 val loss 2.4868\n",
      "step 20100: train loss 2.4805 val loss 2.4807\n",
      "step 20400: train loss 2.4648 val loss 2.5095\n",
      "step 20700: train loss 2.4633 val loss 2.5107\n",
      "step 21000: train loss 2.4715 val loss 2.5125\n",
      "step 21300: train loss 2.4552 val loss 2.4735\n",
      "step 21600: train loss 2.4432 val loss 2.4769\n",
      "step 21900: train loss 2.4814 val loss 2.4771\n",
      "step 22200: train loss 2.4578 val loss 2.4805\n",
      "step 22500: train loss 2.4666 val loss 2.4906\n",
      "step 22800: train loss 2.4572 val loss 2.5031\n",
      "step 23100: train loss 2.4608 val loss 2.4784\n",
      "step 23400: train loss 2.4470 val loss 2.4890\n",
      "step 23700: train loss 2.4645 val loss 2.4791\n",
      "step 24000: train loss 2.4604 val loss 2.5004\n",
      "step 24300: train loss 2.4567 val loss 2.4762\n",
      "step 24600: train loss 2.4347 val loss 2.4833\n",
      "step 24900: train loss 2.4638 val loss 2.4794\n",
      "step 25200: train loss 2.4642 val loss 2.4778\n",
      "step 25500: train loss 2.4784 val loss 2.4886\n",
      "step 25800: train loss 2.4551 val loss 2.4509\n",
      "step 26100: train loss 2.4475 val loss 2.5130\n",
      "step 26400: train loss 2.4834 val loss 2.4747\n",
      "step 26700: train loss 2.4464 val loss 2.4756\n",
      "step 27000: train loss 2.4563 val loss 2.4970\n",
      "step 27300: train loss 2.4500 val loss 2.5113\n",
      "step 27600: train loss 2.4508 val loss 2.4898\n",
      "step 27900: train loss 2.4602 val loss 2.4972\n",
      "step 28200: train loss 2.4656 val loss 2.4957\n",
      "step 28500: train loss 2.4540 val loss 2.4856\n",
      "step 28800: train loss 2.4380 val loss 2.4823\n",
      "step 29100: train loss 2.4685 val loss 2.4943\n",
      "step 29400: train loss 2.4682 val loss 2.4868\n",
      "step 29700: train loss 2.4736 val loss 2.4889\n",
      "step 30000: train loss 2.4592 val loss 2.4908\n",
      "step 30300: train loss 2.4323 val loss 2.4761\n",
      "step 30600: train loss 2.4460 val loss 2.4837\n",
      "step 30900: train loss 2.4609 val loss 2.4697\n",
      "step 31200: train loss 2.4650 val loss 2.4924\n",
      "step 31500: train loss 2.4619 val loss 2.5026\n",
      "step 31800: train loss 2.4557 val loss 2.4746\n",
      "step 32100: train loss 2.4608 val loss 2.5036\n",
      "step 32400: train loss 2.4403 val loss 2.5158\n",
      "step 32700: train loss 2.4425 val loss 2.4767\n",
      "step 33000: train loss 2.4513 val loss 2.4668\n",
      "step 33300: train loss 2.4543 val loss 2.4894\n",
      "step 33600: train loss 2.4415 val loss 2.4893\n",
      "step 33900: train loss 2.4629 val loss 2.5124\n",
      "step 34200: train loss 2.4593 val loss 2.4822\n",
      "step 34500: train loss 2.4424 val loss 2.4801\n",
      "step 34800: train loss 2.4506 val loss 2.4898\n",
      "step 35100: train loss 2.4504 val loss 2.5179\n",
      "step 35400: train loss 2.4828 val loss 2.4911\n",
      "step 35700: train loss 2.4292 val loss 2.5305\n",
      "step 36000: train loss 2.4639 val loss 2.4695\n",
      "step 36300: train loss 2.4542 val loss 2.5192\n",
      "step 36600: train loss 2.4547 val loss 2.4818\n",
      "step 36900: train loss 2.4669 val loss 2.4987\n",
      "step 37200: train loss 2.4619 val loss 2.4758\n",
      "step 37500: train loss 2.4643 val loss 2.5152\n",
      "step 37800: train loss 2.4747 val loss 2.5040\n",
      "step 38100: train loss 2.4838 val loss 2.5094\n",
      "step 38400: train loss 2.4690 val loss 2.4762\n",
      "step 38700: train loss 2.4512 val loss 2.5027\n",
      "step 39000: train loss 2.4809 val loss 2.4928\n",
      "step 39300: train loss 2.4454 val loss 2.5019\n",
      "step 39600: train loss 2.4630 val loss 2.4842\n",
      "step 39900: train loss 2.4711 val loss 2.5054\n",
      "step 40200: train loss 2.4691 val loss 2.4713\n",
      "step 40500: train loss 2.4420 val loss 2.4961\n",
      "step 40800: train loss 2.4679 val loss 2.4854\n",
      "step 41100: train loss 2.4723 val loss 2.4781\n",
      "step 41400: train loss 2.4422 val loss 2.4827\n",
      "step 41700: train loss 2.4559 val loss 2.5014\n",
      "step 42000: train loss 2.4412 val loss 2.5104\n",
      "step 42300: train loss 2.4567 val loss 2.4737\n",
      "step 42600: train loss 2.4683 val loss 2.4946\n",
      "step 42900: train loss 2.4687 val loss 2.4789\n",
      "step 43200: train loss 2.4526 val loss 2.4730\n",
      "step 43500: train loss 2.4660 val loss 2.5054\n",
      "step 43800: train loss 2.4787 val loss 2.5018\n",
      "step 44100: train loss 2.4610 val loss 2.4837\n",
      "step 44400: train loss 2.4206 val loss 2.5064\n",
      "step 44700: train loss 2.4445 val loss 2.5165\n",
      "step 45000: train loss 2.4874 val loss 2.4965\n",
      "step 45300: train loss 2.4509 val loss 2.5063\n",
      "step 45600: train loss 2.4370 val loss 2.5022\n",
      "step 45900: train loss 2.4529 val loss 2.4766\n",
      "step 46200: train loss 2.4712 val loss 2.4900\n",
      "step 46500: train loss 2.4516 val loss 2.4834\n",
      "step 46800: train loss 2.4310 val loss 2.4984\n",
      "step 47100: train loss 2.4480 val loss 2.4977\n",
      "step 47400: train loss 2.4241 val loss 2.5153\n",
      "step 47700: train loss 2.4702 val loss 2.4700\n",
      "step 48000: train loss 2.4539 val loss 2.4547\n",
      "step 48300: train loss 2.4486 val loss 2.4817\n",
      "step 48600: train loss 2.4641 val loss 2.4755\n",
      "step 48900: train loss 2.4460 val loss 2.4565\n",
      "step 49200: train loss 2.4646 val loss 2.4678\n",
      "step 49500: train loss 2.4721 val loss 2.4815\n",
      "step 49800: train loss 2.4742 val loss 2.4808\n",
      "step 50100: train loss 2.4502 val loss 2.5225\n",
      "step 50400: train loss 2.4679 val loss 2.4826\n",
      "step 50700: train loss 2.4830 val loss 2.4981\n",
      "step 51000: train loss 2.4644 val loss 2.4971\n",
      "step 51300: train loss 2.4532 val loss 2.5038\n",
      "step 51600: train loss 2.4597 val loss 2.4829\n",
      "step 51900: train loss 2.4478 val loss 2.4789\n",
      "step 52200: train loss 2.4489 val loss 2.5047\n",
      "step 52500: train loss 2.4379 val loss 2.4699\n",
      "step 52800: train loss 2.4748 val loss 2.4890\n",
      "step 53100: train loss 2.4524 val loss 2.4911\n",
      "step 53400: train loss 2.4723 val loss 2.5166\n",
      "step 53700: train loss 2.4745 val loss 2.4869\n",
      "step 54000: train loss 2.4608 val loss 2.4949\n",
      "step 54300: train loss 2.4497 val loss 2.5116\n",
      "step 54600: train loss 2.4496 val loss 2.4942\n",
      "step 54900: train loss 2.4504 val loss 2.4809\n",
      "step 55200: train loss 2.4374 val loss 2.4993\n",
      "step 55500: train loss 2.4717 val loss 2.4882\n",
      "step 55800: train loss 2.4652 val loss 2.5180\n",
      "step 56100: train loss 2.4274 val loss 2.4929\n",
      "step 56400: train loss 2.4547 val loss 2.4783\n",
      "step 56700: train loss 2.4470 val loss 2.4586\n",
      "step 57000: train loss 2.4532 val loss 2.4828\n",
      "step 57300: train loss 2.4771 val loss 2.5227\n",
      "step 57600: train loss 2.4609 val loss 2.5046\n",
      "step 57900: train loss 2.4610 val loss 2.4984\n",
      "step 58200: train loss 2.4482 val loss 2.4929\n",
      "step 58500: train loss 2.4486 val loss 2.4892\n",
      "step 58800: train loss 2.4399 val loss 2.4735\n",
      "step 59100: train loss 2.4881 val loss 2.4777\n",
      "step 59400: train loss 2.4704 val loss 2.4790\n",
      "step 59700: train loss 2.4720 val loss 2.4887\n",
      "step 60000: train loss 2.4475 val loss 2.4869\n",
      "step 60300: train loss 2.4870 val loss 2.4996\n",
      "step 60600: train loss 2.4610 val loss 2.4917\n",
      "step 60900: train loss 2.4549 val loss 2.5240\n",
      "step 61200: train loss 2.4639 val loss 2.4893\n",
      "step 61500: train loss 2.4772 val loss 2.4979\n",
      "step 61800: train loss 2.4669 val loss 2.4799\n",
      "step 62100: train loss 2.4519 val loss 2.4961\n",
      "step 62400: train loss 2.4462 val loss 2.4841\n",
      "step 62700: train loss 2.4543 val loss 2.4792\n",
      "step 63000: train loss 2.4701 val loss 2.4825\n",
      "step 63300: train loss 2.4600 val loss 2.5031\n",
      "step 63600: train loss 2.4340 val loss 2.5204\n",
      "step 63900: train loss 2.4370 val loss 2.5276\n",
      "step 64200: train loss 2.4512 val loss 2.4962\n",
      "step 64500: train loss 2.4481 val loss 2.5282\n",
      "step 64800: train loss 2.4188 val loss 2.4718\n",
      "step 65100: train loss 2.4637 val loss 2.5151\n",
      "step 65400: train loss 2.4582 val loss 2.5111\n",
      "step 65700: train loss 2.4666 val loss 2.4941\n",
      "step 66000: train loss 2.4543 val loss 2.4882\n",
      "step 66300: train loss 2.4555 val loss 2.4960\n",
      "step 66600: train loss 2.4279 val loss 2.4670\n",
      "step 66900: train loss 2.4698 val loss 2.4654\n",
      "step 67200: train loss 2.4578 val loss 2.4862\n",
      "step 67500: train loss 2.4456 val loss 2.4901\n",
      "step 67800: train loss 2.4628 val loss 2.4794\n",
      "step 68100: train loss 2.4626 val loss 2.4939\n",
      "step 68400: train loss 2.4433 val loss 2.4955\n",
      "step 68700: train loss 2.4483 val loss 2.5012\n",
      "step 69000: train loss 2.4488 val loss 2.4824\n",
      "step 69300: train loss 2.4376 val loss 2.4974\n",
      "step 69600: train loss 2.4766 val loss 2.5091\n",
      "step 69900: train loss 2.4680 val loss 2.4849\n",
      "step 70200: train loss 2.4582 val loss 2.5054\n",
      "step 70500: train loss 2.4423 val loss 2.4990\n",
      "step 70800: train loss 2.4643 val loss 2.4932\n",
      "step 71100: train loss 2.4453 val loss 2.4778\n",
      "step 71400: train loss 2.4510 val loss 2.4713\n",
      "step 71700: train loss 2.4570 val loss 2.4783\n",
      "step 72000: train loss 2.4534 val loss 2.4958\n",
      "step 72300: train loss 2.4488 val loss 2.4931\n",
      "step 72600: train loss 2.4582 val loss 2.4881\n",
      "step 72900: train loss 2.4414 val loss 2.5003\n",
      "step 73200: train loss 2.4457 val loss 2.5026\n",
      "step 73500: train loss 2.4535 val loss 2.5189\n",
      "step 73800: train loss 2.4403 val loss 2.4761\n",
      "step 74100: train loss 2.4418 val loss 2.5104\n",
      "step 74400: train loss 2.4804 val loss 2.4744\n",
      "step 74700: train loss 2.4442 val loss 2.5029\n",
      "step 75000: train loss 2.4425 val loss 2.4870\n",
      "step 75300: train loss 2.4324 val loss 2.4748\n",
      "step 75600: train loss 2.4636 val loss 2.4863\n",
      "step 75900: train loss 2.4499 val loss 2.5054\n",
      "step 76200: train loss 2.4700 val loss 2.4809\n",
      "step 76500: train loss 2.4727 val loss 2.4804\n",
      "step 76800: train loss 2.4462 val loss 2.4960\n",
      "step 77100: train loss 2.4186 val loss 2.5107\n",
      "step 77400: train loss 2.4492 val loss 2.4782\n",
      "step 77700: train loss 2.4614 val loss 2.5100\n",
      "step 78000: train loss 2.4465 val loss 2.5103\n",
      "step 78300: train loss 2.4533 val loss 2.4684\n",
      "step 78600: train loss 2.4665 val loss 2.4884\n",
      "step 78900: train loss 2.4548 val loss 2.4748\n",
      "step 79200: train loss 2.4262 val loss 2.4874\n",
      "step 79500: train loss 2.4430 val loss 2.4619\n",
      "step 79800: train loss 2.4310 val loss 2.4718\n",
      "step 80100: train loss 2.4775 val loss 2.4695\n",
      "step 80400: train loss 2.4700 val loss 2.5164\n",
      "step 80700: train loss 2.4732 val loss 2.4905\n",
      "step 81000: train loss 2.4793 val loss 2.4987\n",
      "step 81300: train loss 2.4679 val loss 2.4943\n",
      "step 81600: train loss 2.4591 val loss 2.4773\n",
      "step 81900: train loss 2.4541 val loss 2.4780\n",
      "step 82200: train loss 2.4624 val loss 2.5073\n",
      "step 82500: train loss 2.4593 val loss 2.5055\n",
      "step 82800: train loss 2.4471 val loss 2.5130\n",
      "step 83100: train loss 2.4657 val loss 2.4956\n",
      "step 83400: train loss 2.4717 val loss 2.5166\n",
      "step 83700: train loss 2.4636 val loss 2.4673\n",
      "step 84000: train loss 2.4401 val loss 2.4921\n",
      "step 84300: train loss 2.4541 val loss 2.4723\n",
      "step 84600: train loss 2.4790 val loss 2.4854\n",
      "step 84900: train loss 2.4794 val loss 2.4974\n",
      "step 85200: train loss 2.4541 val loss 2.5021\n",
      "step 85500: train loss 2.4305 val loss 2.5045\n",
      "step 85800: train loss 2.4643 val loss 2.4886\n",
      "step 86100: train loss 2.4627 val loss 2.4833\n",
      "step 86400: train loss 2.4618 val loss 2.5028\n",
      "step 86700: train loss 2.4605 val loss 2.4915\n",
      "step 87000: train loss 2.4518 val loss 2.5313\n",
      "step 87300: train loss 2.4604 val loss 2.5047\n",
      "step 87600: train loss 2.4452 val loss 2.5149\n",
      "step 87900: train loss 2.4220 val loss 2.4893\n",
      "step 88200: train loss 2.4822 val loss 2.4526\n",
      "step 88500: train loss 2.4438 val loss 2.5316\n",
      "step 88800: train loss 2.4912 val loss 2.4750\n",
      "step 89100: train loss 2.4355 val loss 2.5217\n",
      "step 89400: train loss 2.4367 val loss 2.4962\n",
      "step 89700: train loss 2.4330 val loss 2.4974\n",
      "step 90000: train loss 2.4384 val loss 2.4777\n",
      "step 90300: train loss 2.4577 val loss 2.4643\n",
      "step 90600: train loss 2.4707 val loss 2.4859\n",
      "step 90900: train loss 2.4315 val loss 2.4934\n",
      "step 91200: train loss 2.4296 val loss 2.5095\n",
      "step 91500: train loss 2.4664 val loss 2.4682\n",
      "step 91800: train loss 2.4481 val loss 2.5201\n",
      "step 92100: train loss 2.4846 val loss 2.4892\n",
      "step 92400: train loss 2.4715 val loss 2.4875\n",
      "step 92700: train loss 2.4493 val loss 2.5083\n",
      "step 93000: train loss 2.4645 val loss 2.5219\n",
      "step 93300: train loss 2.4572 val loss 2.4943\n",
      "step 93600: train loss 2.4485 val loss 2.4889\n",
      "step 93900: train loss 2.4476 val loss 2.4985\n",
      "step 94200: train loss 2.4725 val loss 2.4933\n",
      "step 94500: train loss 2.4377 val loss 2.4830\n",
      "step 94800: train loss 2.4593 val loss 2.5084\n",
      "step 95100: train loss 2.4561 val loss 2.5101\n",
      "step 95400: train loss 2.4317 val loss 2.4870\n",
      "step 95700: train loss 2.4572 val loss 2.5001\n",
      "step 96000: train loss 2.4728 val loss 2.5106\n",
      "step 96300: train loss 2.4486 val loss 2.4904\n",
      "step 96600: train loss 2.4435 val loss 2.4948\n",
      "step 96900: train loss 2.4242 val loss 2.4838\n",
      "step 97200: train loss 2.4532 val loss 2.4993\n",
      "step 97500: train loss 2.4856 val loss 2.5097\n",
      "step 97800: train loss 2.4694 val loss 2.4998\n",
      "step 98100: train loss 2.4343 val loss 2.4875\n",
      "step 98400: train loss 2.4767 val loss 2.5264\n",
      "step 98700: train loss 2.4641 val loss 2.5109\n",
      "step 99000: train loss 2.4559 val loss 2.4830\n",
      "step 99300: train loss 2.4431 val loss 2.4606\n",
      "step 99600: train loss 2.4345 val loss 2.5056\n",
      "step 99900: train loss 2.4419 val loss 2.5279\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 32\n",
    "# for steps in range(10000):\n",
    "\n",
    "\n",
    "# print(loss.item())\n",
    "\n",
    "for iter in range(max_iters):\n",
    "  if iter % eval_interval == 0:\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step {iter}: train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
    "    \n",
    "  xb, yb = get_batches('train')\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HEayo in mpery way avend ouburser sickes bokecard dstceny\n",
      "\n",
      "He tw el fe oupise he, lbustselownthous;\n",
      "I m w\n",
      "T:\n",
      "TIONTouly me Ed frks, g he itheland's oe, oghithet f, badogienthofathatey foueay wad,\n",
      "ureisold array n\n",
      "ICoyockind m murs, in mamybalorthyongmyooe, d Vofetthindy st\n",
      "Hefil brveseay alsteanerm to, oupomp rede d pre h, gavitfithrer'GENUpsts lathind my:\n",
      "Berouerse IOLUEDid nghathicerire.\n",
      "In IS:\n",
      "Yok, pequt f keithunghaned t\n",
      "The orerrofe find ans I andoovyonon-hu he nd youlliler pt icis ig y onee\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1) , dtype=torch.long, device=mps_device)\n",
    "print(decode(m.generate( idx = context , max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
